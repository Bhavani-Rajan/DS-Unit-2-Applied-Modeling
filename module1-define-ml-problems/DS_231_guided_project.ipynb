{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "tC6MqtN90A0x",
    "EklA_4VYP31G"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUhphDysw-6P"
   },
   "source": [
    "## BloomTech Data Science\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c9XvBGFMguJ1"
   },
   "source": [
    "# Define ML Problems\n",
    "- Data Leakage\n",
    "- ROC/AUC Curve\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DK_BlRx4va6m"
   },
   "source": [
    "%%capture\n",
    "import sys\n",
    "\n",
    "# If you're on Colab:\n",
    "if 'google.colab' in sys.modules:\n",
    "    DATA_PATH = 'https://raw.githubusercontent.com/bloominstituteoftechnology/DS-Unit-2-Applied-Modeling/master/data/'\n",
    "    !pip install category_encoders==2.*\n",
    "\n",
    "# If you're working locally:\n",
    "else:\n",
    "    DATA_PATH = '../data/'"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kbOK7kIgwAWA"
   },
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, roc_curve, ConfusionMatrixDisplay, RocCurveDisplay, roc_auc_score\n",
    "import matplotlib.pyplot as plt"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_confusion_matrix = ConfusionMatrixDisplay.from_estimator\n",
    "plot_roc_curve = RocCurveDisplay.from_estimator"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "pd.set_option('display.max_columns', 500)"
   ],
   "metadata": {
    "id": "NxBBseZ3wE0g"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vq41fT8Nva6t"
   },
   "source": [
    "# Define ML problems\n",
    "\n",
    "# I. Wrangle Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7rPiEC4Dva6y"
   },
   "source": [
    "def wrangle(filepath):\n",
    "\n",
    "  df = pd.read_csv(filepath)\n",
    "\n",
    "  # # Import w/ DateTimeIndex\n",
    "  # df = pd.read_csv(filepath, parse_dates=['Date'], index_col = 'Date')\n",
    "\n",
    "  # # drop rows with no overall rating\n",
    "  # df.dropna(subset=['overall'], inplace = True)\n",
    "\n",
    "  # # Create `'great'` column as target\n",
    "  # df['great'] = (df['overall'] >= 4).astype(int)\n",
    "\n",
    "  # # Drop overall column to prevent data leakage\n",
    "  # df.drop(columns='overall', inplace = True)\n",
    "\n",
    "  # # Clean binary encoded columns\n",
    "  # categorical_cols = df.select_dtypes('object').columns\n",
    "  # # use categorical columns which are basically binary encoded\n",
    "  # binary_cols = [col for col in categorical_cols if df[col].nunique() < 4]\n",
    "  # for col in binary_cols:\n",
    "  #   df[col] = df[col].apply(lambda x: 1 if isinstance(x, str) else 0)\n",
    "\n",
    "  # # Drop high-cardinality categorical variables\n",
    "  # threshold = 20\n",
    "\n",
    "  # high_card_cols =  [col for col in categorical_columns\n",
    "  #                    if df[col].nunique() > threshold ]\n",
    "  # df.drop(high_card_cols, axis=1, inplace=True)\n",
    "\n",
    "  # # Dropping columns with high number of NaN values\n",
    "  # df.dropna(axis=1, thresh=300, inplace = True)\n",
    "\n",
    "  return df"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "m3NYTxupva6z"
   },
   "source": [
    "df = wrangle(DATA_PATH + 'burritos/burritos.csv')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8rbc3w4va6z"
   },
   "source": [
    "# II. Split Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Jl1Lpcz1va6z"
   },
   "source": [
    "target = ...\n",
    "y = df[target]\n",
    "X = df.drop(target, axis=1)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "y.value_counts(normalize=True).plot(kind='bar')"
   ],
   "metadata": {
    "id": "uo9ClwwFeD45"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "jzOG1xEKeD0Y"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "s4WvxEQQeLAS"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Training\n",
    "train_mask = ...\n",
    "X_train, y_train = X.loc[train_mask], y.loc[train_mask]\n",
    "\n",
    "# Validation\n",
    "validation_mask = ...\n",
    "X_val, y_val = X.loc[validation_mask], y.loc[validation_mask]\n",
    "\n",
    "# Testing\n",
    "test_mask = ...\n",
    "X_test, y_test = X.loc[test_mask], y.loc[test_mask]"
   ],
   "metadata": {
    "id": "7bhMI4L0eK9K"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "prtl34EAva60"
   },
   "source": [
    "# III. Establish Baseline"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0eiKgO3ova61"
   },
   "source": [
    "print('Baseline accuracy:', y_train.value_counts(normalize=True).max())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OqGoXIPEva61"
   },
   "source": [
    "# IV. Build Model"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Model 1: Logistic Regression\n",
    "\n",
    "model_lr = make_pipeline(\n",
    "    SimpleImputer(),\n",
    "    StandardScaler(),\n",
    "    LogisticRegression()\n",
    ")\n",
    "\n",
    "model_lr.fit(X_train, y_train);"
   ],
   "metadata": {
    "id": "ADAlN37Kd3Wx"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Model 2: Random Forest\n",
    "model_rf = make_pipeline(\n",
    "    SimpleImputer(),\n",
    "    RandomForestClassifier(n_jobs=-1)\n",
    ")\n",
    "\n",
    "model_rf.fit(X_train, y_train);"
   ],
   "metadata": {
    "id": "EDfn0lH_d3K1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pPfWN9ezva62"
   },
   "source": [
    "# V. Check Metrics\n",
    "\n",
    "**Accuracy**"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "accuracy = (tp + tn) / (tp + fp + tn + fn)"
   ],
   "metadata": {
    "id": "oOlu9-sUx-uE"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lQg8Ey0sva63"
   },
   "source": [
    "print('Training Accuracy (LOGR):', model_lr.score(X_train, y_train))\n",
    "print('Validation Accuracy (LOGR):', model_lr.score(X_val, y_val))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bJeP3RHAva62"
   },
   "source": [
    "print('Training Accuracy (RF):', model_rf.score(X_train, y_train))\n",
    "print('Validation Accuracy (RF):', model_rf.score(X_val, y_val))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6CeKOr1va63"
   },
   "source": [
    "**Precision, Recall, F1**"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "precision = tp / (tp + fp)\n",
    "\n",
    "\n",
    "recall = tp / (tp + fn)"
   ],
   "metadata": {
    "id": "pebzRbKmDAg4"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print('Logistic Regression')\n",
    "print(classification_report(..., ...))\n",
    "plot_confusion_matrix(..., ..., ...)"
   ],
   "metadata": {
    "id": "TOkMtLBidZ3o"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print('Random Forest')\n",
    "print(classification_report(..., ...))\n",
    "plot_confusion_matrix(..., ..., ...)"
   ],
   "metadata": {
    "id": "uvfPDjTWdZyO"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "6f7u0xK9dZmR"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "E7lRwkUEdZiG"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yEjKr5Clva64"
   },
   "source": [
    "**ROC curve**\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6ElkFY5SkvJr"
   },
   "source": [],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "I5BbPp7F56wW"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Get predicted probabilities from model\n",
    "y_pred_prob = model_rf.predict_proba(X_val)[:, -1]\n",
    "\n",
    "# Input true labels and probability predictions\n",
    "fpr, tpr, thresholds = ...\n",
    "\n",
    "# Put data into dictionary\n",
    "data = {'false_pos_rate': fpr,\n",
    "        'true_pos_rate': tpr,\n",
    "        'thresholds':thresholds}\n",
    "\n",
    "pd.DataFrame(data)"
   ],
   "metadata": {
    "id": "qA5mJEsMdQRW"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tC6MqtN90A0x"
   },
   "source": [
    "## Demonstrating how changing the threshold (from roc-auc curve) changes the metrics"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FYGOfQF-yedm"
   },
   "source": [
    "y_pred_prob[y_pred_prob>= 0.61] = 1\n",
    "y_pred_prob[y_pred_prob < 0.61] = 0"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "A6Pitfgzy_iN"
   },
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print('Validation Accuracy (RF):', accuracy_score(y_val, y_pred_prob))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FFMmvFc4lZqc"
   },
   "source": [
    "print(classification_report(y_val, model_rf.predict(X_val)))\n",
    "plot_confusion_matrix(model_rf, X_val, y_pred_prob);"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EklA_4VYP31G"
   },
   "source": [
    "## Regression Example\n",
    "\n",
    "What if we were predicting the cost of a burrito?\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tl7mWeTTp8L_"
   },
   "source": [
    "# target = 'Cost'\n",
    "# y = df[target]\n",
    "# X = df.drop(columns=target)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Rn5ulfovTZJN"
   },
   "source": [
    "# df['Cost'].hist(bins=20) # to check for target skewness"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lpHSPMQzQWPC"
   },
   "source": [
    "# Radomized Train Test Split\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33, random_state=42)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# from sklearn.metrics import mean_absolute_error\n",
    "# baseline_pred = [y_val.mean()] * len(y_val)\n",
    "# mean_absolute_error(y_val, baseline_pred)"
   ],
   "metadata": {
    "id": "5ZJ-dAk9XgBg"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FLYuAc8tQdzB"
   },
   "source": [
    "\n",
    "# model_lr = make_pipeline(SimpleImputer(),\n",
    "#                       StandardScaler(),\n",
    "#                       LinearRegression())\n",
    "\n",
    "# model_lr.fit(X_train, y_train)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5nfkcZKqSYEq"
   },
   "source": [
    "\n",
    "# model_rf = make_pipeline(SimpleImputer(),\n",
    "#                       RandomForestRegressor())\n",
    "\n",
    "# model_rf.fit(X_train, y_train)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fC8GHIe4SYmd"
   },
   "source": [
    "# print(\"Training Accuracy - Linear Regression\", mean_absolute_error(y_train, model_lr.predict(X_train)))\n",
    "# print(\"Validation Accuracy - Linear Regression\", mean_absolute_error(y_val, model_lr.predict(X_val)))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "z9czTvuKS5Hs"
   },
   "source": [
    "# print(\"Training Accuracy - Random Forest\", mean_absolute_error(y_train, model_rf.predict(X_train)))\n",
    "# print(\"Validation Accuracy - Random Forest\", mean_absolute_error(y_val, model_rf.predict(X_val)))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Log Transformation of skewed Target (for regression)"
   ],
   "metadata": {
    "id": "YhRJzCrYVxmw"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# The chosen target for the above example (burrito cost) is not skewed. But if your regression target is skewed, then you should log transform it, using the following steps:\n",
    "\n",
    "# log transform your target (training set)\n",
    "# y_train_log = np.log1p(y_train)\n",
    "\n",
    "# fit your chosen model to this log tranformed target\n",
    "# model.fit(X_train, y_train_log)\n",
    "\n",
    "# get your predictions for this log tranformed target\n",
    "# y_pred_log = model.predict(X_val)\n",
    "\n",
    "# reverse log tranform your log tranformed target\n",
    "# y_pred = np.expm1(y_pred_log)\n",
    "\n",
    "# check your metrics\n",
    "# print(mean_absolute_error(y_val, y_pred))\n"
   ],
   "metadata": {
    "id": "yyGNUcC9N0cO"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sampling of imbalance data (using .sample function)"
   ],
   "metadata": {
    "id": "b78kDMWtWJZC"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is just one way to over or under sample. Go over these links and read how to implement SMOTE(Synthetic Minority Oversampling Technique) technique too, where you create synthetic observations of the minority class!\n",
    "\n",
    "* https://imbalanced-learn.org/dev/references/generated/imblearn.over_sampling.SMOTE.html\n",
    "* https://www.section.io/engineering-education/imbalanced-data-in-ml/"
   ],
   "metadata": {
    "id": "E8joZRzE4EWW"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jWkWFeDxWNoJ"
   },
   "source": [
    "# # Sampling of imbalance data (using .sample function)\n",
    "\n",
    "# # If train_minority contains only those rows in your training dataset that correspond to minority class, you can oversample like this.\n",
    "# # When you over sample your minority class, you always have to sample with replacement\n",
    "\n",
    "# num_minority_samples = 10 # number of additional minority class rows to create.\n",
    "# train_minority_sample = train_minority.sample(num_minority_samples,replace = True)\n",
    "\n",
    "\n",
    "# # If train_majority contains only those rows in your dataset that correspond to majority class, you can undersample like this.\n",
    "# # Under sampling doesnt need with replacement\n",
    "# num_majority_samples = 10 # number of majority class rows to remove.\n",
    "# train_majority_sample = train_majority.sample(num_majority_samples,replace=False)\n",
    "\n",
    "# # You can then append the two dataframes to form a final train df.\n",
    "# train_final = train_minority_sample.append(train_majority_sample, ignore_index=True)\n"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}
